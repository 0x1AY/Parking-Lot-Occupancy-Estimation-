{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd466107",
   "metadata": {},
   "source": [
    "# Parking Lot Occupancy Estimation - Testing\n",
    "\n",
    "This notebook performs final testing on the test set and generates comprehensive evaluation reports.\n",
    "\n",
    "**Author:** Aminu Yiwere  \n",
    "**Date:** November 4, 2025  \n",
    "**Environment:** Google Colab\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d3279",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa929aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install albumentations\n",
    "!pip install timm\n",
    "!pip install tqdm\n",
    "!pip install matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install plotly\n",
    "!pip install fpdf  # For PDF report generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b107cc",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f69497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "# Additional imports\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score\n",
    ")\n",
    "import timm\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec1363",
   "metadata": {},
   "source": [
    "## 3. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb06f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    if IN_COLAB:\n",
    "        DATA_DIR = '/content/drive/MyDrive/parking_lot_data'\n",
    "        OUTPUT_DIR = '/content/drive/MyDrive/parking_lot_output'\n",
    "    else:\n",
    "        DATA_DIR = './data/processed'\n",
    "        OUTPUT_DIR = './output'\n",
    "    \n",
    "    TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
    "    RESULTS_DIR = os.path.join(OUTPUT_DIR, 'test_results')\n",
    "    \n",
    "    # Model parameters\n",
    "    MODEL_NAME = 'resnet50'  # Should match the trained model\n",
    "    NUM_CLASSES = 2\n",
    "    \n",
    "    # Test parameters\n",
    "    BATCH_SIZE = 32\n",
    "    IMG_SIZE = 224\n",
    "    NUM_WORKERS = 2 if IN_COLAB else 4\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Test directory: {config.TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8419846",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b980a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['occupied', 'vacant']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    label = self.class_to_idx[class_name]\n",
    "                    self.samples.append((img_path, label))\n",
    "        \n",
    "        print(f\"Found {len(self.samples)} images in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label, img_path\n",
    "\n",
    "# Test transforms (no augmentation)\n",
    "def get_test_transforms(img_size=224):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "test_dataset = ParkingLotDataset(\n",
    "    data_dir=config.TEST_DIR,\n",
    "    transform=get_test_transforms(config.IMG_SIZE)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe9ff1",
   "metadata": {},
   "source": [
    "## 5. Load Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, num_classes=2):\n",
    "    \"\"\"Create model architecture\"\"\"\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'resnet101':\n",
    "        model = models.resnet101(pretrained=False)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'vgg16':\n",
    "        model = models.vgg16(pretrained=False)\n",
    "        num_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif 'efficientnet' in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model = create_model(config.MODEL_NAME, config.NUM_CLASSES)\n",
    "checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'best_model_{config.MODEL_NAME}.pth')\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úì Model loaded from {checkpoint_path}\")\n",
    "    print(f\"  Trained for {checkpoint['epoch'] + 1} epochs\")\n",
    "    print(f\"  Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "else:\n",
    "    print(f\"‚ö† Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Please train the model first using train.ipynb\")\n",
    "\n",
    "model = model.to(config.DEVICE)\n",
    "model.eval()\n",
    "print(\"\\nModel ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65979b38",
   "metadata": {},
   "source": [
    "## 6. Test Function with Inference Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Test model with inference time measurement\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_paths = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths in tqdm(dataloader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "            outputs = model(images)\n",
    "            inference_time = time.time() - start_time\n",
    "            inference_times.append(inference_time / images.size(0))  # Time per image\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_paths.extend(paths)\n",
    "    \n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    fps = 1.0 / avg_inference_time if avg_inference_time > 0 else 0\n",
    "    \n",
    "    return (\n",
    "        np.array(all_preds), \n",
    "        np.array(all_labels), \n",
    "        np.array(all_probs), \n",
    "        all_paths,\n",
    "        avg_inference_time,\n",
    "        fps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff8e0e",
   "metadata": {},
   "source": [
    "## 7. Run Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa878c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RUNNING FINAL TEST\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "predictions, labels, probabilities, image_paths, avg_time, fps = test_model(\n",
    "    model, test_loader, config.DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Test complete!\")\n",
    "print(f\"  Total samples: {len(predictions)}\")\n",
    "print(f\"  Average inference time: {avg_time*1000:.2f} ms/image\")\n",
    "print(f\"  Throughput: {fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258f1aa",
   "metadata": {},
   "source": [
    "## 8. Calculate All Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "conf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Per-class metrics\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    labels, predictions, average=None\n",
    ")\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(labels, probabilities[:, 1])\n",
    "\n",
    "# Specificity and Sensitivity\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Overall Performance Metrics:\")\n",
    "print(f\"  ‚Ä¢ Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Precision:   {precision:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall:      {recall:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score:    {f1:.4f}\")\n",
    "print(f\"  ‚Ä¢ ROC AUC:     {roc_auc:.4f}\")\n",
    "print(f\"  ‚Ä¢ Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"  ‚Ä¢ Specificity: {specificity:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Per-Class Performance:\")\n",
    "print(f\"\\n  Occupied (Class 0):\")\n",
    "print(f\"    Precision: {precision_per_class[0]:.4f}\")\n",
    "print(f\"    Recall:    {recall_per_class[0]:.4f}\")\n",
    "print(f\"    F1-Score:  {f1_per_class[0]:.4f}\")\n",
    "print(f\"    Support:   {support[0]}\")\n",
    "\n",
    "print(f\"\\n  Vacant (Class 1):\")\n",
    "print(f\"    Precision: {precision_per_class[1]:.4f}\")\n",
    "print(f\"    Recall:    {recall_per_class[1]:.4f}\")\n",
    "print(f\"    F1-Score:  {f1_per_class[1]:.4f}\")\n",
    "print(f\"    Support:   {support[1]}\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance:\")\n",
    "print(f\"  ‚Ä¢ Inference Time: {avg_time*1000:.2f} ms/image\")\n",
    "print(f\"  ‚Ä¢ Throughput:     {fps:.2f} FPS\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece090d",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names=['Occupied', 'Vacant']):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        square=True,\n",
    "        ax=axes[0],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, pad=15)\n",
    "    axes[0].set_ylabel('True Label', fontsize=12)\n",
    "    axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    # Normalized\n",
    "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(\n",
    "        conf_matrix_normalized,\n",
    "        annot=True,\n",
    "        fmt='.2%',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        square=True,\n",
    "        ax=axes[1],\n",
    "        cbar_kws={'label': 'Percentage'}\n",
    "    )\n",
    "    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, pad=15)\n",
    "    axes[1].set_ylabel('True Label', fontsize=12)\n",
    "    axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f599f",
   "metadata": {},
   "source": [
    "## 10. ROC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f294f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(labels, probabilities):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, probabilities[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=13)\n",
    "    plt.ylabel('True Positive Rate', fontsize=13)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set', fontsize=15, pad=20)\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'roc_curve.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(labels, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239223f",
   "metadata": {},
   "source": [
    "## 11. Sample Predictions Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15975311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(num_correct=6, num_incorrect=6):\n",
    "    \"\"\"\n",
    "    Visualize correct and incorrect predictions\n",
    "    \"\"\"\n",
    "    correct_indices = np.where(predictions == labels)[0]\n",
    "    incorrect_indices = np.where(predictions != labels)[0]\n",
    "    \n",
    "    class_names = ['Occupied', 'Vacant']\n",
    "    \n",
    "    # Correct predictions\n",
    "    if len(correct_indices) > 0:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        sample_indices = np.random.choice(correct_indices, min(num_correct, len(correct_indices)), replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            img = cv2.imread(image_paths[idx])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            confidence = probabilities[idx][predictions[idx]]\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(\n",
    "                f\"‚úì {class_names[predictions[idx]]}\\nConfidence: {confidence:.2%}\",\n",
    "                fontsize=11,\n",
    "                color='green'\n",
    "            )\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Correct Predictions', fontsize=16, y=1.00)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, 'correct_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Incorrect predictions\n",
    "    if len(incorrect_indices) > 0:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        sample_indices = np.random.choice(incorrect_indices, min(num_incorrect, len(incorrect_indices)), replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            img = cv2.imread(image_paths[idx])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            confidence = probabilities[idx][predictions[idx]]\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(\n",
    "                f\"True: {class_names[labels[idx]]}\\n\"\n",
    "                f\"Pred: {class_names[predictions[idx]]} ({confidence:.2%})\",\n",
    "                fontsize=11,\n",
    "                color='red'\n",
    "            )\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Incorrect Predictions', fontsize=16, y=1.00)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, 'incorrect_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nüéâ Perfect predictions! No errors found.\")\n",
    "\n",
    "visualize_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38756f3c",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "results_summary = {\n",
    "    'Model': config.MODEL_NAME,\n",
    "    'Test Samples': len(labels),\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Inference Time (ms)': avg_time * 1000,\n",
    "    'Throughput (FPS)': fps,\n",
    "    'Correct Predictions': np.sum(predictions == labels),\n",
    "    'Incorrect Predictions': np.sum(predictions != labels),\n",
    "    'True Positives': tp,\n",
    "    'True Negatives': tn,\n",
    "    'False Positives': fp,\n",
    "    'False Negatives': fn\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open(os.path.join(config.RESULTS_DIR, 'test_summary.json'), 'w') as f:\n",
    "    json.dump(results_summary, f, indent=4)\n",
    "\n",
    "# Display as DataFrame\n",
    "summary_df = pd.DataFrame([results_summary]).T\n",
    "summary_df.columns = ['Value']\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string())\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b3a49",
   "metadata": {},
   "source": [
    "## 13. Save Detailed Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'true_label': labels,\n",
    "    'true_label_name': [test_dataset.classes[l] for l in labels],\n",
    "    'predicted_label': predictions,\n",
    "    'predicted_label_name': [test_dataset.classes[p] for p in predictions],\n",
    "    'correct': predictions == labels,\n",
    "    'confidence': [probabilities[i][predictions[i]] for i in range(len(predictions))],\n",
    "    'prob_occupied': probabilities[:, 0],\n",
    "    'prob_vacant': probabilities[:, 1]\n",
    "})\n",
    "\n",
    "# Save detailed results\n",
    "results_csv_path = os.path.join(config.RESULTS_DIR, 'test_results_detailed.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"\\n‚úì Detailed results saved to {results_csv_path}\")\n",
    "\n",
    "# Save classification report\n",
    "report = classification_report(\n",
    "    labels, \n",
    "    predictions, \n",
    "    target_names=['Occupied', 'Vacant'],\n",
    "    digits=4\n",
    ")\n",
    "with open(os.path.join(config.RESULTS_DIR, 'classification_report.txt'), 'w') as f:\n",
    "    f.write(\"PARKING LOT OCCUPANCY ESTIMATION - TEST RESULTS\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Model: {config.MODEL_NAME}\\n\")\n",
    "    f.write(f\"Test Samples: {len(labels)}\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.4f}\\n\\n\")\n",
    "    f.write(report)\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Inference Time: {avg_time*1000:.2f} ms/image\\n\")\n",
    "    f.write(f\"Throughput: {fps:.2f} FPS\\n\")\n",
    "\n",
    "print(f\"‚úì Classification report saved to classification_report.txt\")\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': list(results_summary.keys()),\n",
    "    'Value': list(results_summary.values())\n",
    "})\n",
    "metrics_df.to_csv(os.path.join(config.RESULTS_DIR, 'test_metrics.csv'), index=False)\n",
    "print(f\"‚úì Metrics saved to test_metrics.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL RESULTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults directory: {config.RESULTS_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  ‚Ä¢ test_results_detailed.csv\")\n",
    "print(\"  ‚Ä¢ test_metrics.csv\")\n",
    "print(\"  ‚Ä¢ test_summary.json\")\n",
    "print(\"  ‚Ä¢ classification_report.txt\")\n",
    "print(\"  ‚Ä¢ confusion_matrix.png\")\n",
    "print(\"  ‚Ä¢ roc_curve.png\")\n",
    "print(\"  ‚Ä¢ correct_predictions.png\")\n",
    "print(\"  ‚Ä¢ incorrect_predictions.png\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fac38",
   "metadata": {},
   "source": [
    "## 14. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2853085",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\")\n",
    "print(\"‚ïî\" + \"=\"*58 + \"‚ïó\")\n",
    "print(\"‚ïë\" + \" \"*15 + \"FINAL TEST SUMMARY\" + \" \"*25 + \"‚ïë\")\n",
    "print(\"‚ï†\" + \"=\"*58 + \"‚ï£\")\n",
    "print(f\"‚ïë  Model: {config.MODEL_NAME:<47} ‚ïë\")\n",
    "print(f\"‚ïë  Test Samples: {len(labels):<42} ‚ïë\")\n",
    "print(\"‚ï†\" + \"=\"*58 + \"‚ï£\")\n",
    "print(f\"‚ïë  üéØ Accuracy:     {accuracy:.4f} ({accuracy*100:>6.2f}%)\" + \" \"*18 + \"‚ïë\")\n",
    "print(f\"‚ïë  üìä Precision:    {precision:.4f}\" + \" \"*35 + \"‚ïë\")\n",
    "print(f\"‚ïë  üìà Recall:       {recall:.4f}\" + \" \"*35 + \"‚ïë\")\n",
    "print(f\"‚ïë  ‚ö° F1-Score:     {f1:.4f}\" + \" \"*35 + \"‚ïë\")\n",
    "print(f\"‚ïë  üìâ ROC AUC:      {roc_auc:.4f}\" + \" \"*35 + \"‚ïë\")\n",
    "print(\"‚ï†\" + \"=\"*58 + \"‚ï£\")\n",
    "print(f\"‚ïë  ‚è±Ô∏è  Inference:    {avg_time*1000:>6.2f} ms/image\" + \" \"*24 + \"‚ïë\")\n",
    "print(f\"‚ïë  üöÄ Throughput:   {fps:>6.2f} FPS\" + \" \"*30 + \"‚ïë\")\n",
    "print(\"‚ï†\" + \"=\"*58 + \"‚ï£\")\n",
    "print(f\"‚ïë  ‚úì Correct:      {np.sum(predictions == labels):<6} ({np.sum(predictions == labels)/len(labels)*100:>5.2f}%)\" + \" \"*22 + \"‚ïë\")\n",
    "print(f\"‚ïë  ‚úó Incorrect:    {np.sum(predictions != labels):<6} ({np.sum(predictions != labels)/len(labels)*100:>5.2f}%)\" + \" \"*22 + \"‚ïë\")\n",
    "print(\"‚ïö\" + \"=\"*58 + \"‚ïù\")\n",
    "\n",
    "print(\"\\nüéâ Testing complete! All results have been saved.\")\n",
    "print(f\"üìÅ Check {config.RESULTS_DIR} for detailed outputs.\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
