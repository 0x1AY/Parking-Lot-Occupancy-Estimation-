{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c9bcce",
   "metadata": {},
   "source": [
    "# Parking Lot Occupancy Estimation - Training\n",
    "\n",
    "This notebook trains deep learning models for parking lot occupancy estimation.\n",
    "\n",
    "**Author:** Aminu Yiwere  \n",
    "**Date:** November 4, 2025  \n",
    "**Environment:** Google Colab\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9776d1",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages and mount Google Drive (if using Colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install albumentations\n",
    "!pip install timm  # PyTorch Image Models\n",
    "!pip install tqdm\n",
    "!pip install matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8a6e6",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Additional imports\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import timm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0358033",
   "metadata": {},
   "source": [
    "## 3. Configuration and Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1eac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    if IN_COLAB:\n",
    "        DATA_DIR = '/content/drive/MyDrive/parking_lot_data'  # Adjust this path\n",
    "        OUTPUT_DIR = '/content/drive/MyDrive/parking_lot_output'\n",
    "    else:\n",
    "        DATA_DIR = './data/processed'\n",
    "        OUTPUT_DIR = './output'\n",
    "    \n",
    "    TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "    VAL_DIR = os.path.join(DATA_DIR, 'validation')\n",
    "    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
    "    LOG_DIR = os.path.join(OUTPUT_DIR, 'logs')\n",
    "    \n",
    "    # Model parameters\n",
    "    MODEL_NAME = 'resnet50'  # Options: resnet50, resnet101, efficientnet_b0, vgg16\n",
    "    NUM_CLASSES = 2  # occupied, vacant\n",
    "    PRETRAINED = True\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # Image parameters\n",
    "    IMG_SIZE = 224\n",
    "    NUM_WORKERS = 2 if IN_COLAB else 4\n",
    "    \n",
    "    # Training settings\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    LR_SCHEDULER_PATIENCE = 5\n",
    "    LR_SCHEDULER_FACTOR = 0.1\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(config.LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Device: {config.DEVICE}\")\n",
    "print(f\"  Model: {config.MODEL_NAME}\")\n",
    "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {config.NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9874cef",
   "metadata": {},
   "source": [
    "## 4. Dataset Class and Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Directory with 'occupied' and 'vacant' subdirectories\n",
    "            transform: Albumentations transform pipeline\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['occupied', 'vacant']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    label = self.class_to_idx[class_name]\n",
    "                    self.samples.append((img_path, label))\n",
    "        \n",
    "        print(f\"Found {len(self.samples)} images in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        labels = [label for _, label in self.samples]\n",
    "        return np.bincount(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebfb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Pipeline\n",
    "def get_train_transforms(img_size=224):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        A.GaussNoise(p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(img_size=224):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7a0aa",
   "metadata": {},
   "source": [
    "## 5. Load and Explore Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38432e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ParkingLotDataset(\n",
    "    data_dir=config.TRAIN_DIR,\n",
    "    transform=get_train_transforms(config.IMG_SIZE)\n",
    ")\n",
    "\n",
    "val_dataset = ParkingLotDataset(\n",
    "    data_dir=config.VAL_DIR,\n",
    "    transform=get_val_transforms(config.IMG_SIZE)\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Class distribution\n",
    "train_dist = train_dataset.get_class_distribution()\n",
    "val_dist = val_dataset.get_class_distribution()\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"  Training - Occupied: {train_dist[0]}, Vacant: {train_dist[1]}\")\n",
    "print(f\"  Validation - Occupied: {val_dist[0]}, Vacant: {val_dist[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0eaee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def visualize_samples(dataset, num_samples=8):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        img, label = dataset[idx]\n",
    "        \n",
    "        # Denormalize image for visualization\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {dataset.classes[label]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample Training Images:\")\n",
    "visualize_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15829c",
   "metadata": {},
   "source": [
    "## 6. Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f344553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, num_classes=2, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create a model for parking lot occupancy classification\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model architecture\n",
    "        num_classes: Number of output classes\n",
    "        pretrained: Whether to use pretrained weights\n",
    "    \n",
    "    Returns:\n",
    "        model: PyTorch model\n",
    "    \"\"\"\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=pretrained)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'resnet101':\n",
    "        model = models.resnet101(pretrained=pretrained)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'vgg16':\n",
    "        model = models.vgg16(pretrained=pretrained)\n",
    "        num_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'efficientnet_b0':\n",
    "        model = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=num_classes)\n",
    "    \n",
    "    elif model_name == 'efficientnet_b3':\n",
    "        model = timm.create_model('efficientnet_b3', pretrained=pretrained, num_classes=num_classes)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model(config.MODEL_NAME, config.NUM_CLASSES, config.PRETRAINED)\n",
    "model = model.to(config.DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel: {config.MODEL_NAME}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcebc4",
   "metadata": {},
   "source": [
    "## 7. Loss Function and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8cec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=config.LR_SCHEDULER_FACTOR,\n",
    "    patience=config.LR_SCHEDULER_PATIENCE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=config.LOG_DIR)\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a4f2c",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary'\n",
    "    )\n",
    "    \n",
    "    return epoch_loss, epoch_acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff8f92",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_precision': [],\n",
    "    'val_recall': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, config.DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_precision, val_recall, val_f1 = validate_epoch(\n",
    "        model, val_loader, criterion, config.DEVICE\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_precision'].append(val_precision)\n",
    "    history['val_recall'].append(val_recall)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "    writer.add_scalar('Metrics/precision', val_precision, epoch)\n",
    "    writer.add_scalar('Metrics/recall', val_recall, epoch)\n",
    "    writer.add_scalar('Metrics/f1', val_f1, epoch)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"  Val Precision: {val_precision:.4f} | Val Recall: {val_recall:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'best_model_{config.MODEL_NAME}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'history': history\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  ✓ Model saved to {checkpoint_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\n⚠ Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504eccbd",
   "metadata": {},
   "source": [
    "## 10. Plot Training History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29488e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train Acc')\n",
    "    axes[0, 1].plot(history['val_acc'], label='Val Acc')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    axes[1, 0].plot(history['val_precision'], label='Precision')\n",
    "    axes[1, 0].plot(history['val_recall'], label='Recall')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Validation Precision and Recall')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[1, 1].plot(history['val_f1'], label='F1 Score', color='green')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "    axes[1, 1].set_title('Validation F1 Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.OUTPUT_DIR, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a07388e",
   "metadata": {},
   "source": [
    "## 11. Save Final Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history to CSV\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv(os.path.join(config.OUTPUT_DIR, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved to {config.OUTPUT_DIR}/training_history.csv\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Final Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Validation Loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_acc']):.4f}\")\n",
    "print(f\"Best Validation F1 Score: {max(history['val_f1']):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
