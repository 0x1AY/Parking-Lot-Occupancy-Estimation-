{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696f91a1",
   "metadata": {},
   "source": [
    "# Parking Lot Occupancy Estimation - Validation\n",
    "\n",
    "This notebook evaluates trained models on the validation set with detailed metrics and visualizations.\n",
    "\n",
    "**Author:** Aminu Yiwere  \n",
    "**Date:** November 4, 2025  \n",
    "**Environment:** Google Colab\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112e4b2",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ef331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install albumentations\n",
    "!pip install timm\n",
    "!pip install tqdm\n",
    "!pip install matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b385b4",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83497f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "# Additional imports\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score\n",
    ")\n",
    "import timm\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d239a9",
   "metadata": {},
   "source": [
    "## 3. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    if IN_COLAB:\n",
    "        DATA_DIR = '/content/drive/MyDrive/parking_lot_data'\n",
    "        OUTPUT_DIR = '/content/drive/MyDrive/parking_lot_output'\n",
    "    else:\n",
    "        DATA_DIR = './data/processed'\n",
    "        OUTPUT_DIR = './output'\n",
    "    \n",
    "    VAL_DIR = os.path.join(DATA_DIR, 'validation')\n",
    "    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
    "    RESULTS_DIR = os.path.join(OUTPUT_DIR, 'validation_results')\n",
    "    \n",
    "    # Model parameters\n",
    "    MODEL_NAME = 'resnet50'  # Should match the trained model\n",
    "    NUM_CLASSES = 2\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    BATCH_SIZE = 32\n",
    "    IMG_SIZE = 224\n",
    "    NUM_WORKERS = 2 if IN_COLAB else 4\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14683b",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['occupied', 'vacant']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    label = self.class_to_idx[class_name]\n",
    "                    self.samples.append((img_path, label))\n",
    "        \n",
    "        print(f\"Found {len(self.samples)} images in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label, img_path\n",
    "\n",
    "# Validation transforms\n",
    "def get_val_transforms(img_size=224):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "val_dataset = ParkingLotDataset(\n",
    "    data_dir=config.VAL_DIR,\n",
    "    transform=get_val_transforms(config.IMG_SIZE)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9b7b0",
   "metadata": {},
   "source": [
    "## 5. Load Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3192d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, num_classes=2):\n",
    "    \"\"\"Create model architecture\"\"\"\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'resnet101':\n",
    "        model = models.resnet101(pretrained=False)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'vgg16':\n",
    "        model = models.vgg16(pretrained=False)\n",
    "        num_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    elif 'efficientnet' in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model = create_model(config.MODEL_NAME, config.NUM_CLASSES)\n",
    "checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'best_model_{config.MODEL_NAME}.pth')\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Model loaded from {checkpoint_path}\")\n",
    "    print(f\"  Trained for {checkpoint['epoch'] + 1} epochs\")\n",
    "    print(f\"  Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "else:\n",
    "    print(f\"⚠ Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Please train the model first using train.ipynb\")\n",
    "\n",
    "model = model.to(config.DEVICE)\n",
    "model.eval()\n",
    "print(\"\\nModel ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c59725",
   "metadata": {},
   "source": [
    "## 6. Evaluation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_paths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths in tqdm(dataloader, desc='Evaluating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_paths.extend(paths)\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs), all_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466afbb",
   "metadata": {},
   "source": [
    "## 7. Run Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "predictions, labels, probabilities, image_paths = evaluate_model(model, val_loader, config.DEVICE)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n",
    "print(f\"Total samples evaluated: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5ce87",
   "metadata": {},
   "source": [
    "## 8. Calculate Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "conf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Calculate per-class metrics\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    labels, predictions, average=None\n",
    ")\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(labels, probabilities[:, 1])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "print(f\"  Class 0 (Occupied):\")\n",
    "print(f\"    Precision: {precision_per_class[0]:.4f}\")\n",
    "print(f\"    Recall:    {recall_per_class[0]:.4f}\")\n",
    "print(f\"    F1-Score:  {f1_per_class[0]:.4f}\")\n",
    "print(f\"    Support:   {support[0]}\")\n",
    "\n",
    "print(f\"\\n  Class 1 (Vacant):\")\n",
    "print(f\"    Precision: {precision_per_class[1]:.4f}\")\n",
    "print(f\"    Recall:    {recall_per_class[1]:.4f}\")\n",
    "print(f\"    F1-Score:  {f1_per_class[1]:.4f}\")\n",
    "print(f\"    Support:   {support[1]}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929df5b",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a806b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names=['Occupied', 'Vacant']):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with annotations\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(\n",
    "        conf_matrix_normalized,\n",
    "        annot=True,\n",
    "        fmt='.2%',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Percentage'}\n",
    "    )\n",
    "    \n",
    "    # Add counts\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            plt.text(j + 0.5, i + 0.7, f'({conf_matrix[i, j]})',\n",
    "                    ha='center', va='center', color='gray', fontsize=10)\n",
    "    \n",
    "    plt.title('Confusion Matrix (Normalized)', fontsize=16, pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185267e8",
   "metadata": {},
   "source": [
    "## 10. ROC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dce9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(labels, probabilities):\n",
    "    \"\"\"\n",
    "    Plot ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, probabilities[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16, pad=20)\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'roc_curve.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(labels, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95b200",
   "metadata": {},
   "source": [
    "## 11. Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce58497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(\n",
    "    labels, \n",
    "    predictions, \n",
    "    target_names=['Occupied', 'Vacant'],\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Save to file\n",
    "with open(os.path.join(config.RESULTS_DIR, 'classification_report.txt'), 'w') as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf75c0",
   "metadata": {},
   "source": [
    "## 12. Error Analysis - Misclassified Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410fd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "misclassified_indices = np.where(predictions != labels)[0]\n",
    "print(f\"\\nTotal misclassified samples: {len(misclassified_indices)}\")\n",
    "print(f\"Misclassification rate: {len(misclassified_indices)/len(labels)*100:.2f}%\")\n",
    "\n",
    "# Visualize some misclassified samples\n",
    "def visualize_misclassified(indices, num_samples=12):\n",
    "    if len(indices) == 0:\n",
    "        print(\"No misclassified samples found!\")\n",
    "        return\n",
    "    \n",
    "    num_samples = min(num_samples, len(indices))\n",
    "    sample_indices = np.random.choice(indices, num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    class_names = ['Occupied', 'Vacant']\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        img_path = image_paths[idx]\n",
    "        true_label = labels[idx]\n",
    "        pred_label = predictions[idx]\n",
    "        confidence = probabilities[idx][pred_label]\n",
    "        \n",
    "        # Load and display image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(\n",
    "            f\"True: {class_names[true_label]}\\n\"\n",
    "            f\"Pred: {class_names[pred_label]} ({confidence:.2%})\",\n",
    "            fontsize=10,\n",
    "            color='red'\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Misclassified Samples', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'misclassified_samples.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    visualize_misclassified(misclassified_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d7c37",
   "metadata": {},
   "source": [
    "## 13. Confidence Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e690a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence distribution\n",
    "correct_confidences = [probabilities[i][predictions[i]] for i in range(len(predictions)) if predictions[i] == labels[i]]\n",
    "incorrect_confidences = [probabilities[i][predictions[i]] for i in range(len(predictions)) if predictions[i] != labels[i]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Correct predictions\n",
    "axes[0].hist(correct_confidences, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title(f'Confidence Distribution - Correct Predictions\\n(n={len(correct_confidences)})', fontsize=12)\n",
    "axes[0].set_xlabel('Confidence', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].axvline(np.mean(correct_confidences), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {np.mean(correct_confidences):.3f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Incorrect predictions\n",
    "if len(incorrect_confidences) > 0:\n",
    "    axes[1].hist(incorrect_confidences, bins=30, color='red', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title(f'Confidence Distribution - Incorrect Predictions\\n(n={len(incorrect_confidences)})', fontsize=12)\n",
    "    axes[1].set_xlabel('Confidence', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].axvline(np.mean(incorrect_confidences), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {np.mean(incorrect_confidences):.3f}')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No incorrect predictions!', ha='center', va='center', fontsize=14)\n",
    "    axes[1].set_xlim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.RESULTS_DIR, 'confidence_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"  Correct predictions - Mean: {np.mean(correct_confidences):.4f}, Std: {np.std(correct_confidences):.4f}\")\n",
    "if len(incorrect_confidences) > 0:\n",
    "    print(f\"  Incorrect predictions - Mean: {np.mean(incorrect_confidences):.4f}, Std: {np.std(incorrect_confidences):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5e58d",
   "metadata": {},
   "source": [
    "## 14. Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9563665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'true_label': labels,\n",
    "    'predicted_label': predictions,\n",
    "    'correct': predictions == labels,\n",
    "    'confidence': [probabilities[i][predictions[i]] for i in range(len(predictions))],\n",
    "    'prob_occupied': probabilities[:, 0],\n",
    "    'prob_vacant': probabilities[:, 1]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_csv_path = os.path.join(config.RESULTS_DIR, 'validation_results.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"\\n✓ Results saved to {results_csv_path}\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_dict = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC'],\n",
    "    'Score': [accuracy, precision, recall, f1, roc_auc]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "metrics_csv_path = os.path.join(config.RESULTS_DIR, 'metrics_summary.csv')\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "print(f\"✓ Metrics summary saved to {metrics_csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"All results saved to: {config.RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add0889",
   "metadata": {},
   "source": [
    "## 15. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {config.MODEL_NAME}\")\n",
    "print(f\"Total Samples: {len(labels)}\")\n",
    "print(f\"Correct Predictions: {np.sum(predictions == labels)} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Incorrect Predictions: {np.sum(predictions != labels)} ({(1-accuracy)*100:.2f}%)\")\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  • Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  • Precision: {precision:.4f}\")\n",
    "print(f\"  • Recall:    {recall:.4f}\")\n",
    "print(f\"  • F1-Score:  {f1:.4f}\")\n",
    "print(f\"  • ROC AUC:   {roc_auc:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
